{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Survival Chances of Glioma Patients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glioma is a type of tumor that starts in the glial cells of the brain or the spine. Gliomas comprise about 30 percent of all brain tumors and central nervous system tumours, and 80 percent of all malignant brain tumours. [[1]](#[1]) Since it is such a common type of brain tumor, it is important to predict the chances of survival of a patient in order to plan further treatment startegies at the time of diagnosis. Most patients are diagnosed based on Computed Tomography (CT) or Magnetic Resonance Imaging (MRI) scans. In this project, we use a Convolutional Neural Network (CNN) model to analyze these scans and predict whether the patient will survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our initial research, we came across a few papers of interest which employed deep learning techniques to analyze medical imaging. A couple of them are listed briefly below:\n",
    "- Use of Artificial Neural Netowrks (ANN) to predict survival of patients who were diagnosed with Amyotrophic Lateral Sclerosis (ALS) based on MRI scans. [[2]](#[2]) \n",
    "- Evaluation of deep learning networks for predicting clinical outcomes through analyzing time series CT images of patients with locally advanced non–small cell lung cancer (NSCLC). [[3]](#[3])\n",
    "- Brain Tumor Segmentation and Survival Prediction Using Multimodal MRI Scans With Deep Learning [[4]](#[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following topics are covered in the project:\n",
    "1. [Importing the necessary modules](#Import-Modules)\n",
    "2. [Data Collection](#Data-Collection)\n",
    "    1. [Get all the collections from the TCIA API](#Get-all-the-collections-from-the-TCIA-API)\n",
    "    2. [Map the Body part values affected by the collections name](#Map-the-Body-part-values-affected-by-the-collections-name)\n",
    "    3. [Get the collections with only brain images](#Get-the-collections-with-only-brain-images)\n",
    "    4. [Get the collections with clinical data and download the clinical data zip file](#Get-the-collections-with-clinical-data-and-download-the-clinical-data-zip-file)\n",
    "    5. [From the clinical data, get the dataframes with vital status](#From-the-clinical-data,-get-the-dataframes-with-vital-status)\n",
    "    6. [Get the patient ids and their outcomes](#Get-the-patient-ids-and-their-outcomes)\n",
    "    7. [From the patient IDs and collections, get the series id](#From-the-patient-IDs-and-collections,-get-the-series-id)\n",
    "    8. [From the series ids, get the first ten images in that series file](#From-the-series-ids,-get-the-first-ten-images-in-that-series-file)\n",
    "3. [Data Preprocessing](#Data-Preprocessing)\n",
    "4. [CNN Model](#CNN-Model)\n",
    "5. [Conclusions](#Conclusions)\n",
    "6. [Future Work](#Future-Work)\n",
    "7. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests,json,zipfile,pandas,io,tqdm,multiprocessing,matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "import pydicom\n",
    "import tqdm\n",
    "from pydicom import dcmread\n",
    "from pydicom.filebase import DicomBytesIO\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from skimage.transform import resize\n",
    "from torch.nn import Linear, ReLU, BCELoss, Conv2d, Module, Sigmoid\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our entire dataset was collected from the TCIA API. \n",
    "\n",
    "**The Cancer Imaging Archive (TCIA) is a service which de-identifies and hosts a large archive of medical images of cancer accessible for public download.**[[5]](#[5])\n",
    "\n",
    "The data are organized as “collections”; typically patients’ imaging related by a common disease (e.g. lung cancer), image modality or type (MRI, CT, digital histopathology, etc) or research focus. DICOM is the primary file format used by TCIA for radiology imaging. Supporting data related to the images such as patient outcomes, treatment details, genomics and expert analyses are also provided when available.\n",
    "\n",
    "At first, we gathered the names of all the available collections in the API. The following function does that.\n",
    "\n",
    "### Get all the collections from the TCIA API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCollections():\n",
    "    '''Provides all the collections available in the Archive'''\n",
    "    baseurl = 'https://services.cancerimagingarchive.net/services/v3/TCIA'\n",
    "    queryEndpoint = '/query/getCollectionValues?'\n",
    "    queryParams = ''\n",
    "    form = 'format=json'\n",
    "    url = baseurl+queryEndpoint+queryParams+form\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code==200:\n",
    "        Collections = []\n",
    "        for dictionary in response.json():\n",
    "            Collections.append(dictionary['Collection'])\n",
    "        return Collections\n",
    "    else:\n",
    "        raise ValueError('Bad/No response')\n",
    "\n",
    "AllCollections = getCollections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we map each collection to its corresponding body part(s). We used the multiprocessing domain to speed up our code by running it on multiple processors. The wall time required for this code to run without mutiprocessing was 10 mins 59 seconds while with multiprocessing, it took only 2 mins 48 seconds to run.   \n",
    "\n",
    "### Map the Body part values affected by the collections name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DON'T RUN'''\n",
    "import gbp #the function stored as a .py file\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = pool.map(gbp.getBodyPart,[c for c in AllCollections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DON'T RUN'''\n",
    "pooledBodyPartAffected = dict(zip(AllCollections,results))\n",
    "BodyPartAffected = pooledBodyPartAffected\n",
    "%store BodyPartAffected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r BodyPartAffected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stored the mapped results an a dictionary in disk using the store magic function available in python[[6]](#[6]). This helped us to call the variables without running the above code multiple times. \n",
    "\n",
    "The objective of mapping each collection to the affected body part was to isolate the collections which has brain tumor patient information from other collections. Next we filtered out those collections with brain tumor patients.  \n",
    "\n",
    "### Get the collections with only brain images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_collections():\n",
    "    '''returns only those collections where the body part affected is\n",
    "    either the brain or the lung'''\n",
    "    Collections_brain = []\n",
    "    for key,value in BodyPartAffected.items():\n",
    "        if len(value)==1:\n",
    "            if value[0] == 'BRAIN':\n",
    "                Collections_brain.append(key)\n",
    "    return Collections_brain\n",
    "\n",
    "brain = filter_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the collections with brain images, we found only those collections which has clinical data information of patients. Clinical Data contains the outcome of the patient which we will use as our labels later while training the model. The clinical data can be obtained as a zipped file. We cannot obtain the clinical data directly using the API. Therefore, we parsed the webpage using BeuatifulSoup to collect it and downloaded the zipped file. We have only looked at one collection because of computational constraints. But the method can easily be scaled up to all the collections that are available in the TCIA archive.\n",
    "\n",
    "### Get the collections with clinical data and download the clinical data zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bharath will add the function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded the clinical data as a zipfile and used pandas to store that information as a dataframe. Next, we selected only those patients who have vital status information or their survival information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the clinical data, get the dataframes with vital status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, we have randomly chosen to work with the TCGA-LGG collection** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfile = 'TCGA-LGG Clinical Data 1516.zip'\n",
    "def getdfs(file):\n",
    "    with zipfile.ZipFile(file) as thezip:\n",
    "        dataframes = []\n",
    "        for filename in thezip.namelist():\n",
    "            data = pd.read_csv(thezip.open(filename),sep= '\\t',header=[0,1,2])\n",
    "            dataframes.append(data)\n",
    "        dataframes_valid = []\n",
    "    for i,j in enumerate(dataframes):\n",
    "        for column in j.columns:\n",
    "            if 'vital_status' in column:\n",
    "                dataframes_valid.append(j)\n",
    "    return dataframes_valid\n",
    "\n",
    "dfs = getdfs(zfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataframes, we obtained the patient identification numbers along with their outcomes. We will use the outcomes as labels for our model and the patient ids to query for images which we will use as inputs to our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the patient ids and their outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatientID(df):\n",
    "    PIDS = []\n",
    "    for pid in df['bcr_patient_barcode'].values:\n",
    "        PIDS.append(pid[0])\n",
    "    labels = []\n",
    "    for label in df.loc[:,'vital_status'].values:\n",
    "        labels.append(label[0])\n",
    "    return PIDS,labels\n",
    "\n",
    "PID,labels = getPatientID(dfs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each patient id has multiple series ids that contains CT/MRI scanned images of the patients. These series ids are required to query the images.\n",
    "\n",
    "We used multiprocessing to speed up our code and stored the variable using store magic method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the patient IDs and collections, get the series id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DON'T RUN'''\n",
    "import gSID #the function in .py file\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = pool.starmap(gSID.getSeriesID,[(pid,brain[3]) for pid in PID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DON'T RUN'''\n",
    "pooledpdict = {}\n",
    "\n",
    "for pid,label,sid in zip(PID,labels,results):\n",
    "    pooledpdict[pid] = {'label':label,'SerialIDs':sid}\n",
    "\n",
    "%store pooledpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pooledpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdict = pooledpdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the series ids, we can query for the images for that patient. We stored only the first ten images from the API. Without multiprocessing, this function takes more than 8 hours to run (might also run out of memory eventually) but with multiprocessing it only took 1 hour 16 minutes and 3 seconds. We also tried running it in Microsoft Azure but the virtual machine that we created gave a memory error. You will have to create a really expensive VM if you wish to run it in Azure.   \n",
    "\n",
    "### From the series ids, get the first ten images in that series file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DON'T RUN'''\n",
    "\n",
    "import gI #the function in .py file\n",
    "\n",
    "results = pool.map(gI.getImages,[pdict[pid]['SerialIDs'] for pid in PID])\n",
    "pool.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are in dicom format and we parsed those images using the pydicom module.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DON'T RUN'''\n",
    "\n",
    "for pid,imgdata in zip(PID,results):\n",
    "    imagedataset = []\n",
    "    for f in imgdata:\n",
    "        try:\n",
    "            img = dcmread(DicomBytesIO(f))\n",
    "            imagedataset.append(img.pixel_array)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    pdict[pid]['images'] = imagedataset \n",
    "    pdict[pid]['image_count'] = len(imagedataset)\n",
    "    \n",
    "%store pdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stored the patient data as a dictionary called pdict. The keys of the dictionary corresponds to the patient ids. Each key points to another dictionary which has the following keys:\n",
    "1. label: This contains the outcome of the patient\n",
    "2. SerialIDs: This key is used to query images from the API\n",
    "3. images: This has the images of the patient stored as a pixel array\n",
    "4. image_count: the number of stored images for each patient\n",
    "\n",
    "The dictionary was parsed as a pandas dataframe. We made two different dataframes one for training the model and the other to test it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(pdict):\n",
    "    #forming a dataframe\n",
    "    df = pd.DataFrame(pdict).T \n",
    "    \n",
    "    #forming a dataframe by mapping each image of a patient to its label and patient id\n",
    "    def func(idx):\n",
    "        return list(zip(cycle([idx]),df.loc[idx,'images'],cycle([df.loc[idx,'label']])))\n",
    "    \n",
    "    df_all = list(map(func,df.index))\n",
    "    spreadlist = sum(df_all,[])\n",
    "    new_df = pd.DataFrame(spreadlist,columns=['PatientID','Image','label'])\n",
    "    \n",
    "    #splitting that new dataframe into train and test\n",
    "    total = len(new_df)\n",
    "    total_idx = np.random.permutation(total)\n",
    "    test = int(np.ceil(0.2*total))\n",
    "    test_idx = total_idx[:test]\n",
    "    train_idx = total_idx[test:]\n",
    "    \n",
    "    dftrain = new_df.iloc[train_idx]\n",
    "    dftest = new_df.iloc[test_idx]\n",
    "    \n",
    "    return dftrain,dftest \n",
    "\n",
    "dftr,dfte = preprocessing(pdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we stored the dataframes as pickle files [[9]](#[9]). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"train_csv.pkl\"\n",
    "dftr.to_pickle(file_name)\n",
    "\n",
    "file_name_test = \"test_data.pkl\"\n",
    "dfte.to_pickle(file_name_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualized the data to gain more insight about the dataset and found three types of images which we defined as follows:\n",
    "\n",
    "1. Black images: Those images without any information\n",
    "2. RGB scale images: Those images that are not in grayscale format\n",
    "3. Normal images: The images in grayscale format that we can use to train the model \n",
    "\n",
    "Two images from each of the above category is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print black images\n",
    "count = 1\n",
    "for i,image in enumerate(dftr['Image']):\n",
    "    if np.max(image)==0:\n",
    "        if count<3:\n",
    "            plt.figure()\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print rgb images\n",
    "count = 1\n",
    "for i,image in enumerate(dftr['Image']):\n",
    "    if len(image.shape)>2:\n",
    "        if count<3:\n",
    "            plt.figure()\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "        count +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print normal scanned images\n",
    "count = 1\n",
    "for i,image in enumerate(dftr['Image']):\n",
    "    if len(image.shape)<=2 and np.max(image)!=0:\n",
    "        if count<3:\n",
    "            plt.figure()\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "        count +=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only worked with the normal images. We resized each image to 60\\*60 size and got rid of black or rgb images using our process function shown below. We created a new dataframe with 3 columns:\n",
    "1. patient id\n",
    "2. Image\n",
    "3. label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    \n",
    "    #get rid of black images\n",
    "    df.drop(df[df['Image'].map(np.max) == 0].index,inplace=True)\n",
    "    \n",
    "    #get rid of rgb images\n",
    "    def func(img):\n",
    "        if len(img.shape)>2:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    df.drop(df[df['Image'].map(func)].index,inplace=True)\n",
    "    \n",
    "    \n",
    "    #process all the available images; to grayscale,60*60,normalize,proper dimension for pytorch \n",
    "    def processImage(px_data):\n",
    "        \n",
    "        px_data_scaled = px_data / px_data.max()\n",
    "        px_data_scaled = resize(px_data, (60, 60), anti_aliasing=True)\n",
    "        px_data_scaled = px_data_scaled[None,:,:]\n",
    "        return px_data_scaled    \n",
    "    \n",
    "    \n",
    "    df.loc[:,'Image']=df.apply(lambda x: processImage(x['Image']), axis=1)\n",
    "    \n",
    "    #make the labels into bool Alive is True\n",
    "    df.loc[:,'label'] = df['label'].map(lambda x: x == 'Alive')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used pytorch[[7]](#[7]) to train our model. The methods below are similar to HW-5 (deep learning assignment). We ran our code on an nvidia gpu to speed up the execution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Pet images dataset.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_dataframe(df):\n",
    "\n",
    "        data = torch.as_tensor(np.array(df['Image'].tolist(),dtype=np.float32))\n",
    "        labels = np.array(df['label'],dtype=np.int8)[:,None]\n",
    "\n",
    "        \n",
    "        return Dataset(data, labels)\n",
    "\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        return {'label':self.labels[idx],'image':self.data[idx]}\n",
    "    \n",
    "def Dataset_load(df):\n",
    "    return Dataset.from_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset):\n",
    "\n",
    "    total = len(dataset)\n",
    "    test = int(np.ceil(0.2*total))\n",
    "    train = total-test\n",
    "\n",
    "    \n",
    "    return torch.utils.data.random_split(dataset,[train,test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN model architecture is as follows:\n",
    "\n",
    "1. An input layer with 1 channel and 60\\*60 size \n",
    "2. A convolutional 2d layer with relu activation, 12 out channels and a kernel size of 5\n",
    "3. An average pool layer with a kernel_size of 2 and stride 2\n",
    "4. A second convolutional 2d layer with tanh activation, 12 out channels and a kernel size of 5\n",
    "5. Another average pool layer with a kernel_size of 2 and stride 2\n",
    "6. A linear fully connected layer with sigmoid activation. \n",
    "\n",
    "The inspiration behind this architecture is obtained from the LeNet-5 architecture developed by LeCun et al.[[8]](#[8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,num_layers,kernelsize):\n",
    "        '''num_layers: tuple of size 2, one for each convolutional layer, kernel_size: int'''\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, num_layers[0], kernelsize)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.avg1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(num_layers[0], num_layers[1], kernelsize)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.avg2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.outconv1 = (60-kernelsize) + 1\n",
    "        self.outavg1 = (self.outconv1 - 2)/2 + 1\n",
    "        self.outconv2 = self.outavg1 -kernelsize + 1\n",
    "        self.outavg2 = (self.outconv2 - 2)/2 + 1\n",
    "        self.lincount = int(num_layers[1]*self.outavg2**2)\n",
    "        self.fc1   = nn.Linear(self.lincount, 1)\n",
    "        self.sigm1 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X, debug=False):\n",
    "        if debug: print(f\"Input Shape: {X.shape}\")\n",
    "\n",
    "        X = self.avg1(self.relu1(self.conv1(X)))\n",
    "        if debug: print(f\"Conv1 Shape: {X.shape}\")\n",
    "            \n",
    "        X = self.avg2(self.tanh2(self.conv2(X)))\n",
    "        if debug: print(f\"Conv1 Shape: {X.shape}\")\n",
    "\n",
    "        X = X.view(X.size(0), -1) # Flatten the shape\n",
    "        if debug: print(f\"Flattened Shape: {X.shape}\")\n",
    "\n",
    "        X = self.sigm1(self.fc1(X))\n",
    "        if debug: print(f\"Output Shape: {X.shape}\")\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use binary cross entropy loss and the Adam optimizer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataset, learning_rate = 0.001, epochs=25, batch_size=500):\n",
    "    \"\"\" Train the model on data\n",
    "    \"\"\"\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "#     pass # TODO: Set up\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch in range(epochs):\n",
    "        # If you add the training loss to this variable, it will be printed for you\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for data in train_dataloader:\n",
    "            output = model(data['image'].cuda())\n",
    "            loss = criterion(output,data['label'].float().cuda())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_loss+= loss.item()\n",
    "#         pass # TODO:Process all data for each epoch\n",
    "\n",
    "        epoch += 1\n",
    "        if epoch % 50 == 0:\n",
    "            curr_time = time.perf_counter() - start_time\n",
    "            print(f'[{curr_time:6.1f}/{curr_time/epoch*epochs:6.1f}] Epoch {epoch: <3d} loss: {epoch_loss / len(train_dataloader)} acc: {test_model(model, train_dataset)}')\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function given below is used to test our model. We not only take accuracy but also the number of correctly predicted alive/dead cases into account. This is because due to imbalanced dataset (the number of alive cases are much higher than the number of dead cases) accuracy or even f1 score is not a very good evaluation metric of this model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, batch_size=500):\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "    num_correct = 0\n",
    "    num_alive = 0\n",
    "    num_alive_correct = 0\n",
    "    num_dead = 0\n",
    "    num_dead_correct = 0\n",
    "    for data in test_dataloader:\n",
    "        y_pred = model(data[\"image\"].cuda()).round()\n",
    "        y_actual = data[\"label\"].float().cuda()\n",
    "        num_correct += (y_pred == y_actual).sum()\n",
    "        for ya,yp in zip(y_actual,y_pred):\n",
    "            if ya == 1.0:\n",
    "                num_alive+=1\n",
    "                if yp == 1.0:\n",
    "                    num_alive_correct+=1\n",
    "            else:\n",
    "                num_dead+=1\n",
    "                if yp ==0.0:\n",
    "                    num_dead_correct+=1\n",
    "    return num_correct.item() / len(test_data),(num_alive_correct/num_alive),(num_dead_correct/num_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(filename,num_layers,kernel_size,learning_rate,epoch,batchsize):\n",
    "    train_df = pd.read_pickle(filename)\n",
    "    df_train = process(train_df)\n",
    "    print(len(df_train))\n",
    "    ds = Dataset_load(df_train)\n",
    "    train_data, valid_data = tuple(split(ds))\n",
    "\n",
    "    model = Model(num_layers,kernel_size)\n",
    "    model.cuda()\n",
    "    training_loop(model, train_data,learning_rate, epochs = epoch,batch_size = batchsize)\n",
    "\n",
    "    train_acc,alive_acc,dead_acc = test_model(model, train_data)\n",
    "    print(f\"Train accuracy: {train_acc} Alive accuracy: {alive_acc} Dead accuracy: {dead_acc}\")\n",
    "\n",
    "    test_acc,talive_acc,tdead_acc = test_model(model, valid_data)\n",
    "    print(f\"Test accuracy: {test_acc} Alive accuracy: {talive_acc} Dead accuracy: {tdead_acc}\")\n",
    "    \n",
    "    return model,[train_acc,alive_acc,dead_acc],[test_acc,talive_acc,tdead_acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran a grid search on three vital hyperparameters and selected the best result obtained on the validation set. The hyperparameters are:\n",
    "\n",
    "1. number of output channels of the convolutional layers\n",
    "2. learning rate\n",
    "3. number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DON'T RUN'''\n",
    "def hyperparameterOptimization(filename, num_layers,kernelsize,learning_rates, epochs, batch_size = 100):\n",
    "    max_dead_acc = 0\n",
    "    best = [(0,0),0,0]\n",
    "    for num_layer in num_layers:\n",
    "        for learning_rate in learning_rates:\n",
    "            for epoch in epochs:\n",
    "                myModel,train_results,test_results = trainModel(\"train_csv.pkl\",num_layer,kernelsize,learning_rate,epoch,batch_size)\n",
    "                if test_results[2]>max_dead_acc:\n",
    "                    max_dead_acc = test_results[2]\n",
    "                    best = [num_layer,learning_rate,epoch]\n",
    "    return best\n",
    "\n",
    "\n",
    "layers = [(12,12),(12,30),(30,30)]\n",
    "lrs = [0.1,0.01,0.001]\n",
    "epochs = [100,200]\n",
    "\n",
    "choices = hyperparameterOptimization(\"train_csv.pkl\", layers,5,lrs, epochs, batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the best set of hyperparameters as follows:\n",
    "\n",
    "1. 12 output channel for both the 1st and 2nd convolutional layer.\n",
    "2. 0.01 learning rate\n",
    "3. 200 training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [(12, 12), 0.01, 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we trained the model with the best set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel,tracc,teacc = trainModel(\"train_csv.pkl\",choices[0],5,choices[1],choices[2],100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '[1]'>[1]</a> https://en.wikipedia.org/wiki/Glioma\n",
    "\n",
    "<a id = '[2]'>[2]</a> https://www.researchgate.net/publication/309182563_Deep_learning_predictions_of_survival_based_on_MRI_in_amyotrophic_lateral_sclerosis\n",
    "\n",
    "<a id = '[3]'>[3]</a> https://clincancerres.aacrjournals.org/content/25/11/3266\n",
    "\n",
    "<a id = '[4]'>[4]</a> https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6707136/\n",
    "\n",
    "<a id = '[5]'>[5]</a> \n",
    "https://www.cancerimagingarchive.net/\n",
    "\n",
    "<a id = '[6]'>[6]</a> \n",
    "https://ipython.readthedocs.io/en/stable/config/extensions/storemagic.html\n",
    "\n",
    "<a id = '[7]'>[7]</a> \n",
    "https://pytorch.org/\n",
    "\n",
    "<a id = '[8]'>[8]</a>\n",
    "http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n",
    "\n",
    "<a id = '[9]'>[9]</a>\n",
    "https://docs.python.org/3/library/pickle.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
