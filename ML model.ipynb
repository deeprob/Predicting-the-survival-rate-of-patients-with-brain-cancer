{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "import pandas as pd \n",
    "from torch.nn import Linear, ReLU, BCELoss, Conv2d, Module, Sigmoid\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce 940MX'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    \n",
    "    #get rid of black images\n",
    "    df.drop(df[df['Image'].map(np.max) == 0].index,inplace=True)\n",
    "    \n",
    "    #get rid of rgb images\n",
    "    def func(img):\n",
    "        if len(img.shape)>2:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    df.drop(df[df['Image'].map(func)].index,inplace=True)\n",
    "    \n",
    "    \n",
    "    #process all the available images; to grayscale,60*60,normalize,proper dimension for pytorch \n",
    "    def processImage(px_data):\n",
    "        \n",
    "        px_data_scaled = px_data / px_data.max()\n",
    "        px_data_scaled = resize(px_data, (60, 60), anti_aliasing=True)\n",
    "        px_data_scaled = px_data_scaled[None,:,:]\n",
    "        return px_data_scaled    \n",
    "    \n",
    "    \n",
    "    df.loc[:,'Image']=df.apply(lambda x: processImage(x['Image']), axis=1)\n",
    "    \n",
    "    #make the labels into bool Alive is True\n",
    "    df.loc[:,'label'] = df['label'].map(lambda x: x == 'Alive')\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Pet images dataset.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_dataframe(df):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            petimages : PetImages -- The PetImages object containing the images.\n",
    "        kwargs:\n",
    "            size : int -- the size of the canonicalized images.\n",
    "        \"\"\"\n",
    "        data = torch.as_tensor(np.array(df['Image'].tolist(),dtype=np.float32))\n",
    "        labels = np.array(df['label'],dtype=np.int8)[:,None]\n",
    "\n",
    "        \n",
    "        return Dataset(data, labels)\n",
    "\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        # Don't change the constructor\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return the number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Return the element corresponding to idx.\n",
    "        \n",
    "        args:\n",
    "            idx : int -- the index of the sample to return\n",
    "            \n",
    "        returns: Dict -- A dictionary with two elements; \"label\" and \"image\". \"label\" has the associated label\n",
    "            and \"image\" is a (size, size, 3)\n",
    "        \"\"\"\n",
    "        # Convert it to a regular python int.\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        return {'label':self.labels[idx],'image':self.data[idx]}\n",
    "    \n",
    "def Dataset_load(df):\n",
    "    return Dataset.from_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset):\n",
    "    \"\"\" Split pet into train and test sets.\n",
    "    \n",
    "    args:\n",
    "        pet : PetDataset -- the PetDataset instance to split.\n",
    "\n",
    "    kwargs:\n",
    "        train_count: The number of elements in the training set. The remainder should be in the test set.\n",
    "    \n",
    "    return: List[Dataset] -- the list of [train, test] datasets.\n",
    "    \"\"\"\n",
    "    total = len(dataset)\n",
    "    test = int(np.ceil(0.2*total))\n",
    "    train = total-test\n",
    "\n",
    "    \n",
    "    return torch.utils.data.random_split(dataset,[train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,num_layers,kernelsize):\n",
    "        '''num_layers: tuple of size 2, one for each convolutional layer, kernel_size: int'''\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, num_layers[0], kernelsize)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.avg1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(num_layers[0], num_layers[1], kernelsize)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.avg2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.outconv1 = (60-kernelsize) + 1\n",
    "        self.outavg1 = (self.outconv1 - 2)/2 + 1\n",
    "        self.outconv2 = self.outavg1 -kernelsize + 1\n",
    "        self.outavg2 = (self.outconv2 - 2)/2 + 1\n",
    "        self.lincount = int(num_layers[1]*self.outavg2**2)\n",
    "        self.fc1   = nn.Linear(self.lincount, 1)\n",
    "        self.sigm1 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X, debug=False):\n",
    "        if debug: print(f\"Input Shape: {X.shape}\")\n",
    "\n",
    "        X = self.avg1(self.relu1(self.conv1(X)))\n",
    "        if debug: print(f\"Conv1 Shape: {X.shape}\")\n",
    "            \n",
    "        X = self.avg2(self.tanh2(self.conv2(X)))\n",
    "        if debug: print(f\"Conv1 Shape: {X.shape}\")\n",
    "\n",
    "        X = X.view(X.size(0), -1) # Flatten the shape\n",
    "        if debug: print(f\"Flattened Shape: {X.shape}\")\n",
    "\n",
    "        X = self.sigm1(self.fc1(X))\n",
    "        if debug: print(f\"Output Shape: {X.shape}\")\n",
    "\n",
    "        return X\n",
    "\n",
    "def count_parameters(model):\n",
    "    # Count all trainable parameters,\n",
    "    # from https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([5, 1, 60, 60])\n",
      "Conv1 Shape: torch.Size([5, 30, 28, 28])\n",
      "Conv1 Shape: torch.Size([5, 30, 12, 12])\n",
      "Flattened Shape: torch.Size([5, 4320])\n",
      "Output Shape: torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "mM = Model((30,30),5)\n",
    "X = torch.zeros(5,1,60,60)\n",
    "y = mM(X,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataset, learning_rate = 0.001, epochs=25, batch_size=500):\n",
    "    \"\"\" Train the model on data\n",
    "    \"\"\"\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "#     pass # TODO: Set up\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch in range(epochs):\n",
    "        # If you add the training loss to this variable, it will be printed for you\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for data in train_dataloader:\n",
    "            output = model(data['image'].cuda())\n",
    "            loss = criterion(output,data['label'].float().cuda())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_loss+= loss.item()\n",
    "#         pass # TODO:Process all data for each epoch\n",
    "\n",
    "        epoch += 1\n",
    "        if epoch % 50 == 0:\n",
    "            curr_time = time.perf_counter() - start_time\n",
    "            print(f'[{curr_time:6.1f}/{curr_time/epoch*epochs:6.1f}] Epoch {epoch: <3d} loss: {epoch_loss / len(train_dataloader)} acc: {test_model(model, train_dataset)}')\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, batch_size=500):\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "    num_correct = 0\n",
    "    num_alive = 0\n",
    "    num_alive_correct = 0\n",
    "    num_dead = 0\n",
    "    num_dead_correct = 0\n",
    "    for data in test_dataloader:\n",
    "        y_pred = model(data[\"image\"].cuda()).round()\n",
    "        y_actual = data[\"label\"].float().cuda()\n",
    "        num_correct += (y_pred == y_actual).sum()\n",
    "        for ya,yp in zip(y_actual,y_pred):\n",
    "            if ya == 1.0:\n",
    "                num_alive+=1\n",
    "                if yp == 1.0:\n",
    "                    num_alive_correct+=1\n",
    "            else:\n",
    "                num_dead+=1\n",
    "                if yp ==0.0:\n",
    "                    num_dead_correct+=1\n",
    "    print(num_dead,num_alive)\n",
    "    return num_correct.item() / len(test_data),(num_alive_correct/num_alive),(num_dead_correct/num_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def trainModel(filename,num_layers,kernel_size,learning_rate,epoch,batchsize):\n",
    "    train_df = pd.read_pickle(filename)\n",
    "    df_train = process(train_df)\n",
    "    print(len(df_train))\n",
    "    ds = Dataset_load(df_train)\n",
    "    train_data, valid_data = tuple(split(ds))\n",
    "\n",
    "    model = Model(num_layers,kernel_size)\n",
    "    model.cuda()\n",
    "    training_loop(model, train_data,learning_rate, epochs = epoch,batch_size = batchsize)\n",
    "\n",
    "    train_acc,alive_acc,dead_acc = test_model(model, train_data)\n",
    "    print(f\"Train accuracy: {train_acc} Alive accuracy: {alive_acc} Dead accuracy: {dead_acc}\")\n",
    "\n",
    "    test_acc,talive_acc,tdead_acc = test_model(model, valid_data)\n",
    "    print(f\"Test accuracy: {test_acc} Alive accuracy: {talive_acc} Dead accuracy: {tdead_acc}\")\n",
    "    \n",
    "    return model,[train_acc,alive_acc,dead_acc],[test_acc,talive_acc,tdead_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4345\n",
      "539 2937\n",
      "[  96.2/ 192.3] Epoch 50  loss: 4.300051692553929 acc: (0.8449367088607594, 1.0, 0.0)\n",
      "539 2937\n",
      "[ 195.0/ 195.0] Epoch 100 loss: 4.300051692553929 acc: (0.8449367088607594, 1.0, 0.0)\n",
      "Done.\n",
      "539 2937\n",
      "Train accuracy: 0.8449367088607594 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "131 738\n",
      "Test accuracy: 0.8492520138089759 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "4345\n",
      "538 2938\n",
      "[  97.8/ 391.2] Epoch 50  loss: 4.2846780776977536 acc: (0.8452243958573072, 1.0, 0.0)\n",
      "538 2938\n",
      "[ 197.4/ 394.8] Epoch 100 loss: 4.2846780776977536 acc: (0.8452243958573072, 1.0, 0.0)\n",
      "538 2938\n",
      "[ 297.4/ 396.6] Epoch 150 loss: 4.2846780776977536 acc: (0.8452243958573072, 1.0, 0.0)\n",
      "538 2938\n",
      "[ 396.3/ 396.3] Epoch 200 loss: 4.2846780776977536 acc: (0.8452243958573072, 1.0, 0.0)\n",
      "Done.\n",
      "538 2938\n",
      "Train accuracy: 0.8452243958573072 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "132 737\n",
      "Test accuracy: 0.8481012658227848 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "4345\n",
      "543 2933\n",
      "[  95.4/ 190.8] Epoch 50  loss: 0.25962415933609007 acc: (0.9079401611047181, 0.9924991476304126, 0.45119705340699817)\n",
      "543 2933\n",
      "[ 192.6/ 192.6] Epoch 100 loss: 0.15041770817978042 acc: (0.9548331415420023, 0.9795431299011251, 0.8213627992633518)\n",
      "Done.\n",
      "543 2933\n",
      "Train accuracy: 0.9548331415420023 Alive accuracy: 0.9795431299011251 Dead accuracy: 0.8213627992633518\n",
      "127 742\n",
      "Test accuracy: 0.8607594936708861 Alive accuracy: 0.9177897574123989 Dead accuracy: 0.5275590551181102\n",
      "4345\n",
      "525 2951\n",
      "[  95.2/ 380.7] Epoch 50  loss: 0.2918205742325102 acc: (0.892692750287687, 0.9671297865130464, 0.4742857142857143)\n",
      "525 2951\n",
      "[ 192.0/ 383.9] Epoch 100 loss: 0.24205057408128466 acc: (0.9073647871116226, 0.9488309047780413, 0.6742857142857143)\n",
      "525 2951\n",
      "[ 290.2/ 386.9] Epoch 150 loss: 0.1838837900332042 acc: (0.9263521288837745, 0.9437478820738733, 0.8285714285714286)\n",
      "525 2951\n",
      "[ 390.5/ 390.5] Epoch 200 loss: 0.12366583836930138 acc: (0.9384349827387802, 0.9413758048119282, 0.9219047619047619)\n",
      "Done.\n",
      "525 2951\n",
      "Train accuracy: 0.9384349827387802 Alive accuracy: 0.9413758048119282 Dead accuracy: 0.9219047619047619\n",
      "145 724\n",
      "Test accuracy: 0.810126582278481 Alive accuracy: 0.8494475138121547 Dead accuracy: 0.6137931034482759\n",
      "4345\n",
      "543 2933\n",
      "[  95.8/ 191.7] Epoch 50  loss: 0.35772991010120936 acc: (0.8656501726121979, 0.9952267303102625, 0.16574585635359115)\n",
      "543 2933\n",
      "[ 193.6/ 193.6] Epoch 100 loss: 0.3181271531752178 acc: (0.8823360184119677, 0.9890896692806, 0.30570902394106814)\n",
      "Done.\n",
      "543 2933\n",
      "Train accuracy: 0.8823360184119677 Alive accuracy: 0.9890896692806 Dead accuracy: 0.30570902394106814\n",
      "127 742\n",
      "Test accuracy: 0.8688147295742232 Alive accuracy: 0.9824797843665768 Dead accuracy: 0.2047244094488189\n",
      "4345\n",
      "552 2924\n",
      "[  95.3/ 381.2] Epoch 50  loss: 0.3729163084711347 acc: (0.8639240506329114, 0.9941860465116279, 0.17391304347826086)\n",
      "552 2924\n",
      "[ 192.5/ 385.0] Epoch 100 loss: 0.33625003610338483 acc: (0.8762945914844649, 0.9897400820793434, 0.2753623188405797)\n",
      "552 2924\n",
      "[ 289.0/ 385.4] Epoch 150 loss: 0.30942430198192594 acc: (0.8857882623705409, 0.9852941176470589, 0.358695652173913)\n",
      "552 2924\n",
      "[ 385.8/ 385.8] Epoch 200 loss: 0.28277931553976876 acc: (0.8970080552359033, 0.9866621067031464, 0.4221014492753623)\n",
      "Done.\n",
      "552 2924\n",
      "Train accuracy: 0.8970080552359033 Alive accuracy: 0.9866621067031464 Dead accuracy: 0.4221014492753623\n",
      "118 751\n",
      "Test accuracy: 0.8814729574223246 Alive accuracy: 0.9693741677762983 Dead accuracy: 0.3220338983050847\n",
      "4345\n",
      "544 2932\n",
      "[ 113.2/ 226.5] Epoch 50  loss: 4.309608268737793 acc: (0.8434982738780207, 1.0, 0.0)\n",
      "544 2932\n",
      "[ 228.5/ 228.5] Epoch 100 loss: 4.309608268737793 acc: (0.8434982738780207, 1.0, 0.0)\n",
      "Done.\n",
      "544 2932\n",
      "Train accuracy: 0.8434982738780207 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "126 743\n",
      "Test accuracy: 0.8550057537399309 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "4345\n",
      "532 2944\n",
      "[ 114.0/ 455.8] Epoch 50  loss: 4.222352474076407 acc: (0.8469505178365938, 1.0, 0.0)\n",
      "532 2944\n",
      "[ 229.7/ 459.4] Epoch 100 loss: 4.222352474076407 acc: (0.8469505178365938, 1.0, 0.0)\n",
      "532 2944\n",
      "[ 347.7/ 463.6] Epoch 150 loss: 4.222352474076407 acc: (0.8469505178365938, 1.0, 0.0)\n",
      "532 2944\n",
      "[ 468.1/ 468.1] Epoch 200 loss: 4.222352474076407 acc: (0.8469505178365938, 1.0, 0.0)\n",
      "Done.\n",
      "532 2944\n",
      "Train accuracy: 0.8469505178365938 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "138 731\n",
      "Test accuracy: 0.8411967779056386 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "4345\n",
      "528 2948\n",
      "[ 116.4/ 232.8] Epoch 50  loss: 0.42636257069451466 acc: (0.8486766398158804, 0.9996607869742198, 0.005681818181818182)\n",
      "528 2948\n",
      "[ 237.8/ 237.8] Epoch 100 loss: 0.42579371673720223 acc: (0.8489643268124281, 1.0, 0.005681818181818182)\n",
      "Done.\n",
      "528 2948\n",
      "Train accuracy: 0.8489643268124281 Alive accuracy: 1.0 Dead accuracy: 0.005681818181818182\n",
      "142 727\n",
      "Test accuracy: 0.8423475258918297 Alive accuracy: 0.9986244841815681 Dead accuracy: 0.04225352112676056\n",
      "4345\n",
      "534 2942\n",
      "[ 117.6/ 470.5] Epoch 50  loss: 0.2635148980787822 acc: (0.906789413118527, 0.9830047586675731, 0.4868913857677903)\n",
      "534 2942\n",
      "[ 239.8/ 479.5] Epoch 100 loss: 0.19040002993174962 acc: (0.937571921749137, 0.982324949014276, 0.6910112359550562)\n",
      "534 2942\n",
      "[ 362.4/ 483.2] Epoch 150 loss: 0.12186387138707297 acc: (0.9597238204833142, 0.977906186267845, 0.8595505617977528)\n",
      "534 2942\n",
      "[ 483.7/ 483.7] Epoch 200 loss: 0.09158675830279078 acc: (0.9660529344073648, 0.977906186267845, 0.900749063670412)\n",
      "Done.\n",
      "534 2942\n",
      "Train accuracy: 0.9660529344073648 Alive accuracy: 0.977906186267845 Dead accuracy: 0.900749063670412\n",
      "136 733\n",
      "Test accuracy: 0.856156501726122 Alive accuracy: 0.9113233287858117 Dead accuracy: 0.5588235294117647\n",
      "4345\n",
      "546 2930\n",
      "[ 119.1/ 238.2] Epoch 50  loss: 0.34767441621848516 acc: (0.8708285385500575, 0.9877133105802047, 0.24358974358974358)\n",
      "546 2930\n",
      "[ 241.1/ 241.1] Epoch 100 loss: 0.3025515407323837 acc: (0.8872266973532796, 0.9853242320819112, 0.3608058608058608)\n",
      "Done.\n",
      "546 2930\n",
      "Train accuracy: 0.8872266973532796 Alive accuracy: 0.9853242320819112 Dead accuracy: 0.3608058608058608\n",
      "124 745\n",
      "Test accuracy: 0.8676639815880323 Alive accuracy: 0.9704697986577181 Dead accuracy: 0.25\n",
      "4345\n",
      "531 2945\n",
      "[ 117.7/ 470.9] Epoch 50  loss: 0.3448728523084095 acc: (0.8719792865362486, 0.9911714770797962, 0.21092278719397364)\n",
      "531 2945\n",
      "[ 238.5/ 476.9] Epoch 100 loss: 0.29992814106600624 acc: (0.890391254315305, 0.9884550084889644, 0.3465160075329567)\n",
      "531 2945\n",
      "[ 355.3/ 473.8] Epoch 150 loss: 0.2573715261050633 acc: (0.9105293440736478, 0.9857385398981324, 0.4934086629001883)\n",
      "531 2945\n",
      "[ 470.9/ 470.9] Epoch 200 loss: 0.21719140197549547 acc: (0.9240506329113924, 0.9898132427843803, 0.559322033898305)\n",
      "Done.\n",
      "531 2945\n",
      "Train accuracy: 0.9240506329113924 Alive accuracy: 0.9898132427843803 Dead accuracy: 0.559322033898305\n",
      "139 730\n",
      "Test accuracy: 0.8653624856156502 Alive accuracy: 0.9575342465753425 Dead accuracy: 0.381294964028777\n",
      "4345\n",
      "542 2934\n",
      "[ 183.6/ 367.3] Epoch 50  loss: 4.306284298215593 acc: (0.8440736478711163, 1.0, 0.0)\n",
      "542 2934\n",
      "[ 369.0/ 369.0] Epoch 100 loss: 4.306284298215593 acc: (0.8440736478711163, 1.0, 0.0)\n",
      "Done.\n",
      "542 2934\n",
      "Train accuracy: 0.8440736478711163 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "128 741\n",
      "Test accuracy: 0.8527042577675489 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "4345\n",
      "532 2944\n",
      "[ 183.2/ 732.9] Epoch 50  loss: 4.227338453701564 acc: (0.8469505178365938, 1.0, 0.0)\n",
      "532 2944\n",
      "[ 368.3/ 736.6] Epoch 100 loss: 4.227338453701564 acc: (0.8469505178365938, 1.0, 0.0)\n",
      "532 2944\n",
      "[ 553.1/ 737.5] Epoch 150 loss: 4.227338453701564 acc: (0.8469505178365938, 1.0, 0.0)\n",
      "532 2944\n",
      "[ 737.8/ 737.8] Epoch 200 loss: 4.227338453701564 acc: (0.8469505178365938, 1.0, 0.0)\n",
      "Done.\n",
      "532 2944\n",
      "Train accuracy: 0.8469505178365938 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "138 731\n",
      "Test accuracy: 0.8411967779056386 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "4345\n",
      "540 2936\n",
      "[ 183.2/ 366.3] Epoch 50  loss: 4.283016089030674 acc: (0.8446490218642118, 1.0, 0.0)\n",
      "540 2936\n",
      "[ 368.6/ 368.6] Epoch 100 loss: 4.283016089030674 acc: (0.8446490218642118, 1.0, 0.0)\n",
      "Done.\n",
      "540 2936\n",
      "Train accuracy: 0.8446490218642118 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "130 739\n",
      "Test accuracy: 0.8504027617951668 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "4345\n",
      "531 2945\n",
      "[ 182.8/ 731.1] Epoch 50  loss: 4.216950920649937 acc: (0.8472382048331415, 1.0, 0.0)\n",
      "531 2945\n",
      "[ 368.5/ 737.0] Epoch 100 loss: 4.216950920649937 acc: (0.8472382048331415, 1.0, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531 2945\n",
      "[ 553.6/ 738.1] Epoch 150 loss: 4.216950920649937 acc: (0.8472382048331415, 1.0, 0.0)\n",
      "531 2945\n",
      "[ 739.0/ 739.0] Epoch 200 loss: 4.216950920649937 acc: (0.8472382048331415, 1.0, 0.0)\n",
      "Done.\n",
      "531 2945\n",
      "Train accuracy: 0.8472382048331415 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "139 730\n",
      "Test accuracy: 0.8400460299194477 Alive accuracy: 1.0 Dead accuracy: 0.0\n",
      "4345\n",
      "531 2945\n",
      "[ 182.3/ 364.6] Epoch 50  loss: 0.33459288605621884 acc: (0.8757192174913694, 0.9830220713073005, 0.2806026365348399)\n",
      "531 2945\n",
      "[ 367.0/ 367.0] Epoch 100 loss: 0.28483943726335254 acc: (0.8955696202531646, 0.9748726655348048, 0.455743879472693)\n",
      "Done.\n",
      "531 2945\n",
      "Train accuracy: 0.8955696202531646 Alive accuracy: 0.9748726655348048 Dead accuracy: 0.455743879472693\n",
      "139 730\n",
      "Test accuracy: 0.8814729574223246 Alive accuracy: 0.9671232876712329 Dead accuracy: 0.4316546762589928\n",
      "4345\n",
      "538 2938\n",
      "[ 182.2/ 728.9] Epoch 50  loss: 0.32762346054826463 acc: (0.879746835443038, 0.9833219877467665, 0.3141263940520446)\n",
      "538 2938\n",
      "[ 365.7/ 731.5] Epoch 100 loss: 0.2758966169187001 acc: (0.8970080552359033, 0.9778761061946902, 0.45539033457249073)\n",
      "538 2938\n",
      "[ 550.9/ 734.6] Epoch 150 loss: 0.22370279473917826 acc: (0.9177215189873418, 0.9775357385976855, 0.5910780669144982)\n",
      "538 2938\n",
      "[ 734.1/ 734.1] Epoch 200 loss: 0.1795613544327872 acc: (0.9387226697353279, 0.9799183117767188, 0.7137546468401487)\n",
      "Done.\n",
      "538 2938\n",
      "Train accuracy: 0.9387226697353279 Alive accuracy: 0.9799183117767188 Dead accuracy: 0.7137546468401487\n",
      "132 737\n",
      "Test accuracy: 0.8722669735327964 Alive accuracy: 0.9443690637720489 Dead accuracy: 0.4696969696969697\n"
     ]
    }
   ],
   "source": [
    "def hyperparameterOptimization(filename, num_layers,kernelsize,learning_rates, epochs, batch_size = 100):\n",
    "    max_dead_acc = 0\n",
    "    best = [(0,0),0,0]\n",
    "    for num_layer in num_layers:\n",
    "        for learning_rate in learning_rates:\n",
    "            for epoch in epochs:\n",
    "                myModel,train_results,test_results = trainModel(\"train_csv.pkl\",num_layer,kernelsize,learning_rate,epoch,batch_size)\n",
    "                if test_results[2]>max_dead_acc:\n",
    "                    max_dead_acc = test_results[2]\n",
    "                    best = [num_layer,learning_rate,epoch]\n",
    "    return best\n",
    "\n",
    "\n",
    "layers = [(12,12),(12,30),(30,30)]\n",
    "lrs = [0.1,0.01,0.001]\n",
    "epochs = [100,200]\n",
    "\n",
    "choices = hyperparameterOptimization(\"train_csv.pkl\", layers,5,lrs, epochs, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 12), 0.01, 200]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8625\n",
      "1162 5738\n",
      "[ 186.5/ 746.1] Epoch 50  loss: 0.45215947256572003 acc: (0.8334782608695652, 1.0, 0.011187607573149742)\n",
      "1162 5738\n",
      "[ 377.3/ 754.5] Epoch 100 loss: 0.39689004205275275 acc: (0.8508695652173913, 0.9956430812129662, 0.1359724612736661)\n",
      "1162 5738\n",
      "[ 567.2/ 756.3] Epoch 150 loss: 0.34303287189939746 acc: (0.8697101449275362, 0.9827466016033461, 0.31153184165232356)\n",
      "1162 5738\n",
      "[ 757.9/ 757.9] Epoch 200 loss: 0.3101755320162013 acc: (0.8828985507246376, 0.9872777971418613, 0.3674698795180723)\n",
      "Done.\n",
      "1162 5738\n",
      "Train accuracy: 0.8828985507246376 Alive accuracy: 0.9872777971418613 Dead accuracy: 0.3674698795180723\n",
      "324 1401\n",
      "Test accuracy: 0.8220289855072463 Alive accuracy: 0.9493219129193433 Dead accuracy: 0.2716049382716049\n"
     ]
    }
   ],
   "source": [
    "myModel,tracc,teacc = trainModel(\"train_csv.pkl\",choices[0],5,choices[1],choices[2],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2153\n",
      "394 1759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8392940083604273, 0.9658897100625355, 0.27411167512690354)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_pickle('test_data.pkl')\n",
    "df_test = process(test_df)\n",
    "print(len(df_test))\n",
    "ds = Dataset_load(df_test)\n",
    "test_model(myModel,ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
